{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:28:11.467668Z",
     "start_time": "2020-12-17T15:27:55.132672Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def check_URL(input_string):\n",
    "    print(\"Welcome to IsItDown.py!\")\n",
    "    print(\"Please write a URL or URLs you want to check. (separated by comma)\")\n",
    "    li = input_string.replace(' ','').split(',')\n",
    "    li = ['http://'+i.lower() if 'http://' not in i.lower() else i for i in li ]\n",
    "    for site in li:\n",
    "        try:\n",
    "            if int(requests.get(site).status_code)//100 == 2: \n",
    "                print(f\"{site} is up!\")\n",
    "            else:\n",
    "                print(f\"{site} is down!\")\n",
    "        except:\n",
    "            print(f\"{site} is down!\")\n",
    "\n",
    "def answer():\n",
    "    repeat = input(\"Do you want to start over? y/n \")\n",
    "    if repeat == 'n':\n",
    "        return 0\n",
    "    elif repeat == 'y':\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Wrong Answer. again\")\n",
    "        return answer()\n",
    "            \n",
    "while True:\n",
    "    input_string = input()\n",
    "    check_URL(input_string)\n",
    "    if answer():\n",
    "        continue\n",
    "    else:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-19T14:10:52.411319Z",
     "start_time": "2020-12-19T14:10:41.402186Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.system(\"clear\")\n",
    "\n",
    "url = \"https://www.iban.com/currency-codes\"\n",
    "\n",
    "\n",
    "countries = []\n",
    "\n",
    "request = requests.get(url)\n",
    "soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "\n",
    "table = soup.find(\"table\")\n",
    "rows = table.find_all(\"tr\")[1:]\n",
    "\n",
    "for row in rows:\n",
    "    items = row.find_all(\"td\")\n",
    "    name = items[0].text\n",
    "    code =items[2].text\n",
    "    if name and code:\n",
    "        if name != \"No universal currency\":\n",
    "            country = {\n",
    "                'name':name.capitalize(),\n",
    "                'code': code\n",
    "            }\n",
    "            countries.append(country)\n",
    "\n",
    "\n",
    "def ask():\n",
    "    try:\n",
    "        choice = int(input(\"#: \"))\n",
    "        if choice > len(countries):\n",
    "            print(\"Choose a number from the list.\")\n",
    "            ask()\n",
    "        else:\n",
    "            country = countries[choice]\n",
    "            print(f\"{country['name']}\\n\")\n",
    "    except ValueError:\n",
    "        print(\"That wasn't a number.\")\n",
    "        ask()\n",
    "    return country['code']\n",
    "\n",
    "def ask_convert():\n",
    "    print(f\"\\nHow many {country_a} do you want to convert to {country_b}?\")\n",
    "    money_input = input()\n",
    "    try:\n",
    "        money_input = float(money_input)\n",
    "    except:\n",
    "        print(\"That wasn't a number.\")\n",
    "        money_input = ask_convert()\n",
    "    return money_input\n",
    "\n",
    "def get_sym():\n",
    "    symbol_url = \"https://transferwise.com/gb/blog/world-currency-symbols\"\n",
    "\n",
    "    request = requests.get(symbol_url)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    tables = soup.find_all(\"table\")\n",
    "\n",
    "    rows=[]\n",
    "    for table in tables:\n",
    "        rows.extend(table.find_all(\"tr\")[1:])\n",
    "\n",
    "    code_sym_dict=dict()\n",
    "\n",
    "    for row in rows:\n",
    "        items = row.find_all(\"td\")\n",
    "        code = items[2].text.strip()\n",
    "        sym =items[3].text.strip()\n",
    "        code_sym_dict[code] = sym\n",
    "    return code_sym_dict\n",
    "          \n",
    "def convert(money,sym):\n",
    "    rate_url = f\"https://transferwise.com/gb/currency-converter/{country_a.lower()}-to-{country_b.lower()}-rate\"\n",
    "    request = requests.get(rate_url)\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "    try:\n",
    "        result = money * float(soup.select(\"h3 > span.text-success\")[0].text)\n",
    "    except:\n",
    "          import pdb;pdb.set_trace()\n",
    "    print(f\"{country_a}{money:,.2f} is {sym}{result:,}\")\n",
    "\n",
    "                  \n",
    "print(\"Hello! Please choose select a country by number:\")\n",
    "for index, country in enumerate(countries):\n",
    "    print(f\"#{index} {country['name']}\")\n",
    "\n",
    "print(\"Where are you from? Choose a country by number.\\n\")\n",
    "\n",
    "country_a = ask()\n",
    "print('Now choose another country.\\n')\n",
    "country_b = ask()\n",
    "          \n",
    "code_sym_dict = get_sym()\n",
    "\n",
    "money = ask_convert()\n",
    "\n",
    "try:\n",
    "    sym = code_sym_dict[country_b]\n",
    "except:\n",
    "    sym = '$'\n",
    "\n",
    "convert(money,sym)\n",
    "\n",
    "## 나라랑 코드들을 받으면 유저가 2개의 나라를 선택하게 해.\n",
    "## 유저가 나라a 에서 나라b로 변환하고 싶은 통화 량을 선택하게 해\n",
    "## 2개의 화폐코드랑 양을 URL로 보내\n",
    "##beautiful soup사용해서 Transfer Wise에서 변환 결과확인해서 가져와."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignment 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### http://www.alba.co.kr/ 의 \n",
    "- ```html <ul class=\"goodBox\">``` 에서 아래와같은 컬럼 추출\n",
    "- place, title, time, pay, date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-21T16:56:58.146285Z",
     "start_time": "2020-12-21T16:56:25.950753Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_super_brand(url):\n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "    ## 슈퍼브랜드 li a태그 가져오기\n",
    "    MainSuperBrand = soup.find('div',id='MainSuperBrand')\n",
    "    ul = MainSuperBrand.find('ul',class_='goodsBox')\n",
    "    superBrand_list = ul.find_all('li')\n",
    "\n",
    "    superBrand_list = superBrand_list[:-1]\n",
    "\n",
    "    href_list = []\n",
    "    for superBrand in superBrand_list:\n",
    "        company_name = superBrand.find('span',{'class':'company'}).text\n",
    "        link = superBrand.a['href']\n",
    "        href_list.append({'company':company_name,'link':link})\n",
    "    return href_list\n",
    "\n",
    "def get_table_row_li(super_brand_url):\n",
    "    try:\n",
    "        response = requests.get(super_brand_url['link'])\n",
    "    except:\n",
    "        import pdb;pdb.set_trace()\n",
    "\n",
    "    super_brand_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    ## place, title, time, pay, date\n",
    "    NormalInfo = super_brand_soup.find('div', id='NormalInfo')\n",
    "    table = NormalInfo.find('tbody')\n",
    "    tr_li = table.find_all('tr')\n",
    "    return super_brand_url['company'],tr_li\n",
    "    \n",
    "\n",
    "    \n",
    "def get_job_info(table_row_li):\n",
    "    brand_li=list()\n",
    "    tr_li = table_row_li\n",
    "    for tr in tr_li[::2]:\n",
    "        try:\n",
    "            place = tr.find('td',{'class':'local first'}).text.replace(u'\\xa0', u' ')\n",
    "        except:\n",
    "            import pdb;pdb.set_trace()\n",
    "        title = tr.find('td',{'class':'title'}).find('span',{'class':'company'}).text.strip()\n",
    "        time = tr.find('td',{'class':'data'}).text\n",
    "        pay = tr.find('td',{'class':'pay'}).text\n",
    "        date = tr.find('td',{'class':'regDate last'}).text\n",
    "        brand_li.append({'place':place, 'title':title, 'time':time,'pay':pay,'date':date})\n",
    "    return brand_li\n",
    "\n",
    "os.system(\"clear\")\n",
    "alba_url = \"http://www.alba.co.kr\"\n",
    "\n",
    "href_list = get_super_brand(alba_url)\n",
    "for one_company in href_list:\n",
    "    company_name, table_row_li = get_table_row_li(one_company)\n",
    "    if len(table_row_li) == 1:\n",
    "        continue\n",
    "    brand_li =  get_job_info(table_row_li)\n",
    "    company_name = company_name.replace('/','_')\n",
    "    pd.DataFrame(brand_li)[['place','title','time','pay','date']].to_csv(f'{company_name}.csv',index=False)\n",
    "    print(f\"{company_name} csv 생성완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T15:57:55.329157Z",
     "start_time": "2020-12-23T15:56:41.618611Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from flask import Flask, render_template, request\n",
    "import pandas as pd\n",
    "\n",
    "base_url = \"http://hn.algolia.com/api/v1\"\n",
    "\n",
    "# This URL gets the newest stories.\n",
    "new = f\"{base_url}/search_by_date?tags=story\"\n",
    "\n",
    "# This URL gets the most popular stories\n",
    "popular = f\"{base_url}/search?tags=story\"\n",
    "\n",
    "def extract_json(order_by):\n",
    "    page={}\n",
    "    r_page = requests.get(order_by)\n",
    "    for r in r_page.json()['hits']:\n",
    "        page.update({r['objectID'] : {'num_comments' : r['num_comments'],\n",
    "                    'author' : r['author'],\n",
    "                    'points' : r['points'],\n",
    "                    'title' : r['title'],\n",
    "                    'url' : r['url'],\n",
    "                    'objectID' : r['objectID']\n",
    "                    }})\n",
    "    return page\n",
    "\n",
    "# This function makes the URL to get the detail of a storie by id.\n",
    "# Heres the documentation: https://hn.algolia.com/api\n",
    "def make_detail_url(id):\n",
    "    return f\"{base_url}/items/{id}\"\n",
    "\n",
    "new_page_di = extract_json(new)\n",
    "new_page_li = list(new_page_di.values())\n",
    "popular_page_di = extract_json(popular)\n",
    "popular_page_li = list(popular_page_di.values())\n",
    "\n",
    "app = Flask(\"DayNine\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home(**kwargs):\n",
    "    order_by = request.args.get('order_by')\n",
    "    if order_by:\n",
    "        if order_by == 'new':\n",
    "            return render_template('order_new.html',page_li=new_page_li)\n",
    "        elif order_by == 'popular':\n",
    "            \n",
    "            return render_template('index.html',page_li=popular_page_li)\n",
    "    else:\n",
    "        return render_template('index.html')\n",
    "\n",
    "@app.route(\"/<id>\")\n",
    "def go_id_page(id):\n",
    "    if id in new_page_di.keys():\n",
    "        news = new_page_di[str(id)]\n",
    "    elif id in popular_page_di.keys():\n",
    "        news = popular_page_di[str(id)]\n",
    "    comment_url = make_detail_url(id)\n",
    "    comment_page = requests.get(comment_url)\n",
    "    df = pd.DataFrame(comment_page.json()['children'])\n",
    "    df_comment = df[df.author.notna()][['author','text']]\n",
    "    return render_template('detail.html', df=df_comment, news=news)\n",
    "\n",
    "# @app.route(\"/?order_by=popular\")\n",
    "# def order_popular():\n",
    "#     return render_template(\"index.html\")\n",
    "\n",
    "app.run(host=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Assingment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-25T18:50:31.854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"DayEleven\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [26/Dec/2020 03:50:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [26/Dec/2020 03:50:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [26/Dec/2020 03:51:05] \"\u001b[37mGET /read?golang=on&django=on HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from flask import Flask, render_template, request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "When you try to scrape reddit make sure to send the 'headers' on your request.\n",
    "Reddit blocks scrappers so we have to include these headers to make reddit think\n",
    "that we are a normal computer and not a python script.\n",
    "How to use: requests.get(url, headers=headers)\n",
    "\"\"\"\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "All subreddits have the same url:\n",
    "i.e : https://reddit.com/r/javascript\n",
    "You can add more subreddits to the list, just make sure they exist.\n",
    "To make a request, use this url:\n",
    "https://www.reddit.com/r/{subreddit}/top/?t=month\n",
    "This will give you the top posts in per month.\n",
    "\"\"\"\n",
    "\n",
    "subreddits = [\n",
    "    \"javascript\",\n",
    "    \"reactjs\",\n",
    "    \"reactnative\",\n",
    "    \"programming\",\n",
    "    \"css\",\n",
    "    \"golang\",\n",
    "    \"flutter\",\n",
    "    \"rust\",\n",
    "    \"django\"\n",
    "]\n",
    "\n",
    "\n",
    "app = Flask(\"DayEleven\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home(**kwargs):\n",
    "#     order_by = request.args.get('order_by')\n",
    "    return render_template('home.html')\n",
    "\n",
    "@app.route(\"/read\")\n",
    "def read(**kwargs):\n",
    "    check_li=[]\n",
    "    for sub in subreddits:\n",
    "        check_li.append(request.args.get(sub))\n",
    "        on_li = [subreddits[i] for i in [idx for idx,i in enumerate(check_li) if i=='on']]\n",
    "    result_di={}\n",
    "    for subreddit in on_li:\n",
    "        url = f'https://www.reddit.com/r/{subreddit}/top/?t=month'\n",
    "        r = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        div_all = soup.find('div', class_='rpBJOHq2PR60pnwJlUyP0')\n",
    "        div_li = div_all.find_all('div',class_='_1oQyIsiPHYt6nx7VOmd1sz')\n",
    "        result_di[subreddit] = []\n",
    "        for i in range(len(div_li)):\n",
    "            try:\n",
    "                vote_num_li = div_li[i].find_all('div',{'class':'_1rZYMD_4xY3gRcSS3p8ODO'})\n",
    "                if len(vote_num_li) == 2:\n",
    "                    if vote_num_li[0].text == vote_num_li[1].text:\n",
    "                        vote_num = int(vote_num_li[0].text)\n",
    "                else:\n",
    "                    continue\n",
    "                title = div_li[i].find('h3',class_='_eYtD2XCVieq6emjKBH3m').text\n",
    "                link = 'https://www.reddit.com' + div_li[i].find('a',{'class':'SQnoC3ObvgnGjWt90zD9Z'})['href']\n",
    "                result_di[subreddit].append([title,link,vote_num])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    df_main = pd.DataFrame(columns=['title','link','vote','kind'])\n",
    "\n",
    "    for i in on_li:\n",
    "        df = pd.DataFrame(result_di[i])\n",
    "        df['kind'] = i\n",
    "        df.columns=['title','link','vote','kind']\n",
    "        df_main = pd.concat([df_main,df])\n",
    "\n",
    "    df_main.sort_values('vote',ascending=0,inplace=True)\n",
    "    return render_template('read.html',df=df_main,on_li=on_li)\n",
    "\n",
    "\n",
    "app.run(host=\"0.0.0.0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
